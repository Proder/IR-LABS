{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LZnRfc41J19E"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import fetch_20newsgroups\n",
        "newsgroups = fetch_20newsgroups(subset='all', remove=('headers', 'footers', 'quotes'))\n",
        "documents = newsgroups.data\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Download stopwords and initialize stemmer\n",
        "nltk.download('stopwords')\n",
        "stop_words = set(stopwords.words('english'))\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "# Text preprocessing function\n",
        "def preprocess_text(text):\n",
        "    # Tokenize, lowercase, remove stopwords, and stem\n",
        "    words = word_tokenize(text)\n",
        "    words = [word.lower() for word in words if word.isalpha()]\n",
        "    words = [stemmer.stem(word) for word in words if word not in stop_words]\n",
        "    return ' '.join(words)\n",
        "\n",
        "# Preprocess all documents\n",
        "preprocessed_documents = [preprocess_text(doc) for doc in documents]\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Kq5zWYzKBXp",
        "outputId": "66da9c2e-ec48-4528-b9fd-ff7e4fdb2b3f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "vectorizer = TfidfVectorizer()\n",
        "term_document_matrix = vectorizer.fit_transform(preprocessed_documents)\n"
      ],
      "metadata": {
        "id": "EXR15Ni7LUrJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import TruncatedSVD\n",
        "\n",
        "num_topics = 100\n",
        "lsa = TruncatedSVD(n_components=num_topics)\n",
        "lsi_matrix = lsa.fit_transform(term_document_matrix)\n"
      ],
      "metadata": {
        "id": "vnxBojUdLYLY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "terms = vectorizer.get_feature_names_out()\n",
        "\n",
        "# Print top terms for each topic\n",
        "for i, topic in enumerate(lsa.components_):\n",
        "    top_terms = [terms[j] for j in topic.argsort()[-10:][::-1]]\n",
        "    print(f\"Topic {i+1}: {', '.join(top_terms)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vBfYNirDLbiU",
        "outputId": "1da7c3f4-a624-4745-a417-311295d9a45f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Topic 1: would, use, one, get, like, know, peopl, think, could, time\n",
            "Topic 2: window, file, drive, thank, card, use, program, driver, run, disk\n",
            "Topic 3: game, team, year, drive, play, player, get, car, go, win\n",
            "Topic 4: drive, scsi, disk, ide, control, card, hard, floppi, system, chip\n",
            "Topic 5: key, chip, encrypt, govern, use, clipper, secur, escrow, phone, system\n",
            "Topic 6: thank, pleas, anyon, know, post, would, email, mail, advanc, appreci\n",
            "Topic 7: key, game, god, chip, encrypt, clipper, use, team, system, escrow\n",
            "Topic 8: card, driver, monitor, video, window, color, mode, vga, car, chip\n",
            "Topic 9: card, armenian, peopl, game, israel, govern, pleas, jew, arab, muslim\n",
            "Topic 10: car, sale, new, includ, price, imag, offer, book, list, year\n",
            "Topic 11: file, would, imag, card, monitor, format, color, think, like, video\n",
            "Topic 12: file, card, car, driver, key, know, get, god, bike, chip\n",
            "Topic 13: would, car, god, file, armenian, appreci, price, card, game, like\n",
            "Topic 14: armenian, anyon, know, use, thank, problem, god, work, turkish, muslim\n",
            "Topic 15: get, god, go, would, file, sale, modem, mac, like, monitor\n",
            "Topic 16: use, gun, game, right, law, monitor, peopl, thank, know, printer\n",
            "Topic 17: know, program, anyon, softwar, system, want, good, go, peopl, disk\n",
            "Topic 18: god, gun, driver, problem, get, fire, law, space, fbi, system\n",
            "Topic 19: problem, car, israel, system, game, arab, anyon, isra, jew, get\n",
            "Topic 20: thank, israel, right, system, monitor, god, run, arab, team, color\n",
            "Topic 21: one, problem, file, anyon, year, know, monitor, system, new, team\n",
            "Topic 22: israel, use, arab, year, team, isra, driver, get, jew, player\n",
            "Topic 23: driver, thank, work, good, use, printer, armenian, year, team, peopl\n",
            "Topic 24: get, problem, color, monitor, imag, game, key, drive, geb, chastiti\n",
            "Topic 25: one, bank, geb, game, chastiti, intellect, surrend, gordon, skeptic, shame\n",
            "Topic 26: system, god, monitor, know, armenian, use, post, list, get, like\n",
            "Topic 27: bank, geb, chastiti, intellect, gordon, surrend, run, skeptic, shame, use\n",
            "Topic 28: get, car, one, program, run, gun, number, mac, team, could\n",
            "Topic 29: get, christian, list, homosexu, bike, mail, one, church, law, system\n",
            "Topic 30: get, system, use, could, christian, thank, monitor, anyon, think, file\n",
            "Topic 31: program, use, disk, card, problem, key, like, monitor, bike, run\n",
            "Topic 32: system, say, version, bike, anyon, moral, book, armenian, good, look\n",
            "Topic 33: could, say, program, use, list, go, think, mail, card, car\n",
            "Topic 34: post, bike, one, mac, right, read, monitor, need, anyon, run\n",
            "Topic 35: driver, run, christian, one, list, version, mac, jesu, pleas, peopl\n",
            "Topic 36: look, list, like, book, mail, problem, monitor, find, think, help\n",
            "Topic 37: work, system, post, thank, peopl, key, car, modem, softwar, like\n",
            "Topic 38: go, say, printer, right, port, system, like, disk, font, homosexu\n",
            "Topic 39: homosexu, number, call, look, like, could, phone, monitor, mac, bike\n",
            "Topic 40: think, question, answer, call, need, run, want, ask, phone, version\n",
            "Topic 41: say, tri, work, use, post, jesu, chip, one, simm, system\n",
            "Topic 42: system, bike, font, chip, new, peopl, imag, post, problem, program\n",
            "Topic 43: say, list, need, key, font, problem, gun, peopl, do, israel\n",
            "Topic 44: book, system, could, imag, good, pleas, want, someon, problem, call\n",
            "Topic 45: think, gun, work, anyon, like, jesu, list, new, control, imag\n",
            "Topic 46: say, could, gun, christian, question, mac, monitor, answer, bike, modem\n",
            "Topic 47: question, right, could, like, answer, ask, want, work, time, key\n",
            "Topic 48: monitor, work, program, font, need, gun, homosexu, book, driver, system\n",
            "Topic 49: tri, run, need, printer, number, like, year, anyon, system, say\n",
            "Topic 50: tri, think, go, run, scsi, font, key, homosexu, new, server\n",
            "Topic 51: tri, disk, mac, pleas, question, read, one, monitor, data, softwar\n",
            "Topic 52: go, new, delet, stuff, number, believ, book, thank, make, disk\n",
            "Topic 53: delet, good, stuff, tri, thank, key, gun, space, number, jesu\n",
            "Topic 54: need, tri, good, disk, problem, simm, post, think, could, homosexu\n",
            "Topic 55: go, list, question, make, mail, anyon, imag, answer, time, good\n",
            "Topic 56: test, year, simm, help, port, right, inform, appreci, think, last\n",
            "Topic 57: font, card, look, work, disk, year, christian, post, number, gun\n",
            "Topic 58: year, simm, need, memori, mac, good, delet, color, believ, read\n",
            "Topic 59: year, new, delet, know, right, need, chip, stuff, control, homosexu\n",
            "Topic 60: delet, new, color, question, stuff, run, port, book, drive, gun\n",
            "Topic 61: anyon, go, peopl, question, right, find, answer, year, pleas, softwar\n",
            "Topic 62: delet, do, monitor, stuff, help, scsi, disk, govern, softwar, question\n",
            "Topic 63: chip, book, us, thank, make, go, port, color, offer, read\n",
            "Topic 64: pleas, font, need, delet, new, test, good, like, stuff, number\n",
            "Topic 65: mac, test, delet, call, stuff, want, need, disk, scsi, time\n",
            "Topic 66: want, say, comput, graphic, also, someon, softwar, think, right, chip\n",
            "Topic 67: test, want, disk, look, system, color, govern, encrypt, also, see\n",
            "Topic 68: make, font, want, new, disk, moral, anyon, thing, good, mous\n",
            "Topic 69: good, test, got, disk, chip, like, space, moral, monitor, email\n",
            "Topic 70: us, make, good, christian, say, call, control, test, board, fan\n",
            "Topic 71: us, version, want, much, anyon, well, price, do, need, tell\n",
            "Topic 72: time, simm, softwar, memori, comput, year, modem, law, hi, set\n",
            "Topic 73: test, make, help, someon, got, appreci, price, font, right, space\n",
            "Topic 74: call, color, time, softwar, widget, display, chip, applic, someth, know\n",
            "Topic 75: color, read, line, articl, board, do, pc, two, moral, year\n",
            "Topic 76: comput, make, could, mous, test, line, widget, much, first, color\n",
            "Topic 77: time, read, right, team, could, mous, make, appl, drug, email\n",
            "Topic 78: copi, time, color, address, version, scsi, sure, also, believ, two\n",
            "Topic 79: version, make, do, want, first, call, got, email, believ, test\n",
            "Topic 80: test, find, softwar, mous, copi, price, format, may, chip, team\n",
            "Topic 81: articl, way, call, thank, differ, said, make, ye, encrypt, printer\n",
            "Topic 82: modem, speed, win, christian, graphic, phone, drug, color, mhz, got\n",
            "Topic 83: find, interest, line, phone, may, hi, printer, anybodi, thing, memori\n",
            "Topic 84: board, group, also, read, interest, do, color, player, might, start\n",
            "Topic 85: read, articl, much, first, find, info, faq, inform, appreci, good\n",
            "Topic 86: sure, memori, info, moral, port, kill, call, got, make, want\n",
            "Topic 87: mean, help, thing, hi, team, articl, site, send, time, ftp\n",
            "Topic 88: modem, info, copi, much, memori, address, mode, speed, even, still\n",
            "Topic 89: softwar, take, number, ye, memori, copi, version, look, differ, might\n",
            "Topic 90: also, comput, day, word, find, mean, messag, tell, faq, address\n",
            "Topic 91: much, display, sure, board, give, two, christian, pitch, name, price\n",
            "Topic 92: much, simm, pc, said, graphic, church, find, thing, way, believ\n",
            "Topic 93: ye, lot, name, price, ca, buy, wonder, color, sure, modem\n",
            "Topic 94: point, first, format, mous, anybodi, articl, version, jesu, heard, data\n",
            "Topic 95: articl, seem, much, thing, someon, interest, board, ship, server, mous\n",
            "Topic 96: realli, church, believ, got, time, format, mail, convert, color, team\n",
            "Topic 97: much, port, ask, may, day, ye, code, team, point, mean\n",
            "Topic 98: got, set, pc, mode, email, encrypt, give, address, articl, mac\n",
            "Topic 99: much, see, ye, number, encrypt, help, widget, ca, fan, heard\n",
            "Topic 100: format, day, convert, color, copi, well, memori, sure, point, exist\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XKsnud4mBBI7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Create a query\n",
        "query = \"science and technology\"\n",
        "\n",
        "# Preprocess the query\n",
        "query = preprocess_text(query)\n",
        "\n",
        "# Transform the query into the LSI space\n",
        "query_vector = lsa.transform(vectorizer.transform([query]))\n",
        "\n",
        "# Compute cosine similarity between the query and documents\n",
        "similarities = cosine_similarity(query_vector, lsi_matrix)\n",
        "\n",
        "# Get the most relevant document indices\n",
        "top_indices = similarities[0].argsort()[::-1]\n",
        "\n",
        "# Print the most relevant documents\n",
        "for i in range(5):\n",
        "    print(f\"Document {i + 1}: {documents[top_indices[i]]}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HA5TRl2mLhZ4",
        "outputId": "11ef4faf-7bda-40a8-b52e-5079a5c7f926"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Document 1: -- \n",
            "PAOLO,MARC ANTHONY\n",
            "Georgia Institute of Technology, Atlanta Georgia, 30332\n",
            "uucp:     ...!{allegra,amd,hplabs,ut-ngp}!gatech!prism!gt4661a\n",
            "Internet: gt4661a@prism.gatech.edu\n",
            "\n",
            "Document 2: \n",
            "Gulp.\n",
            "\n",
            "[Disclaimer:  This opinion is mine and does not represent the views of\n",
            "Fermilab, Universities Research Association, the Department of Energy,\n",
            "or the 49th Ward Regular Science Fiction Organization.]\n",
            " \n",
            "Document 3: The following statement was released\n",
            "on February 27,1992 by the Science &\n",
            "Environmental Policy Project\n",
            "\n",
            "As independent scientists researching atmosphere and climate problems, we are \n",
            "concerned by the agenda for UNCED, the United Nations Conference on \n",
            "Environment and Development, being developed by environmental and  activists \n",
            "groups and certain political leaders. This so called \"Earth Summit\" is \n",
            "scheduled to convene in  Brazil in June 1992 and aims to impose a system of \n",
            "global envionmental regulations, including onerous taxes on energy fuels, on \n",
            "the population of the United States and other industrialized nations.\n",
            "Such policy initiatives derive from highly uncetain scientific theories. They \n",
            "are based on the unsupported assumption that catastrophic global warming \n",
            "follows from the burning of fossill fuels and requires immediate action. We \n",
            "do not agree. \n",
            "A survey of U.S. Atmospheric scientists, conducted in the summer of 1991, \n",
            "confirms that there is  no consesensus about the cause of the slight warming \n",
            "observed during the past century. A recently published research paper even \n",
            "suggests sunspot variability (which is directly proportional to solar \n",
            "activity),  rather  than a rise in greenhouse gases is responsible for the \n",
            "global temperature increases and decreases recoded since about 1880.\n",
            "Futhermore, the majority of scientific participants in the survey agreed that \n",
            "the theoretical climate climate models used to predict a future warming \n",
            "cannot be relied upon and are not validated by the existing climate record. \n",
            "Yet all predictions are based on such theoretical models.\n",
            "Finally, agriculturalits generally agree that any increase in carbon dioxide \n",
            "levels from fossil fuels burning has beneficial effects on most crops and on \n",
            "world food supply.\n",
            "We are disturbed that activists, anxious to stop energy and economic growth, \n",
            "are pushing ahead with drastic policies without taking notice of recent \n",
            "changes in the underlying science. We fear that the rush to impose global \n",
            "regulations will have catastrophic impacts on the world economy, on jobs, \n",
            "standards of living, and health care, with the most severe consequences \n",
            "falling on developing countries and the  poor.\n",
            "David B. Aubrey, PhD, Senior Scintist, Woods  Hole Oceanographic Institute. \n",
            "Nathaniel B. Guttman, PhD, Research Physical Scientist, National Climatic \n",
            "Data Center. Hugh B. Ellsaesser, PhD, Meteorologist, Lawerence Livermore \n",
            "National Laboratory. Richard Lindzen, PhD, Center for Meteorology and \n",
            "Physical Meteorolgy, Massachusetts Institute of Technology. Robert C. \n",
            "Balling, PhD, Director, Laboratory of Climatology, Arizona State University.\n",
            "Patrick Micheals, PhD, Assoc. Professor of Environmental Sciences, \n",
            "Universityy of Virginia. Roger Pielke, PhD, Professor of Atmospheric Science, \n",
            "Colorado State University. Micheal Garstang, PhD, Professor of Meteorology, \n",
            "University of Virginia. Sherwood P. Idso, PhD, Research Physicist, U.S. Water \n",
            "Conservation Laboratory.\n",
            "Lev S. Gandin PhD, Visiting Scientist, National Center for Atmospheric \n",
            "Research. John A. McGinley, Chief, Forecast Research group, Forecast Systems \n",
            "Laboratory, NOAA. H. Jean Thiebaux, PhD, Research Scientist, National \n",
            "Meterological Center, National Weather Service, NOAA. Kenneth V. Beard, PhD, \n",
            "Professor of Atmospheric Physics, University of Illinois. Paul W. Mielke, Jr. \n",
            "PhD, Professor, Department of Statistics, Colorado State University. Thomas \n",
            "Lockhart, Meteorological Standards Institute.\n",
            "Peter F. Giddings, Meterologist, Weather Service Director. Hazen A. bedke, \n",
            "Meteoroligist, Former Regional Director, National Weather Service.\n",
            "\n",
            "\n",
            "Gabriel T. Csanady, PhD, Eminent Professor, Old Dominion University. Roy \n",
            "Leep, Executive Weather Director, Gillet Weather Data Services. Terrance J. \n",
            "Clark, Meteorologist, U.S. Air Force. Neil L. Frank, PhD, Meteorologist, \n",
            "National Weather service. Bruce A. Boe, PhD, Director, North Dakota \n",
            "Atmospheric Resource Board. Andrew Detweiler, PhD, Assoc. Professor, \n",
            "Institute of Atmospheric Sciences, South Dakota School of Mines And \n",
            "Technology.\n",
            "Robert M. Cunningham, Consulting Meteorologist, Fellow, American \n",
            "Meteorological Society. Stephen R. Hanna, PhD, Sigma Research Corporation, \n",
            "Elliot Abrams, Meteoroligist, Senior Vice President, AccuWeather, Inc. \n",
            "William E. Reifsnyder, PhD, Consulting Meteorologist, professor Emeritus, \n",
            "Forest Meteorology, Yale University. David W. Reylnolds, Research \n",
            "meteorologist. Jerry A. Williams, Meteorologist, President, Ocean Routes, \n",
            "Inc.\n",
            "Lee W. Eddington, Meteorologist, Geophysics Division, Pacific Missile test \n",
            "Center.Werner A Braum, PhD, Former Dean, College of Arts & Sciences, Florida \n",
            "State University.David P. Rodgers, PhD, Assoc. Professor of Research \n",
            "Oceanography, Scripps Institution of Oceanograghy. Brian Fiedler, PhD, Asst \n",
            "professor of Meteorology, University of Oaklahoma.\n",
            "Edward A. Brandes, Meterologist. Melvyn Shapiro, Chief of Meteorological \n",
            "Research Wave Propagation Laboratory, NOAA. Joesph Zabransky, Jr., Associate \n",
            "professor of Meteorology, Plymouth State College. James A. Moore, Project \n",
            "Manager, Research Applications program, national Center for Atmospheric \n",
            "Research. Daniel J McNaughton, ENSR Consultating and Engineering. Brian \n",
            "Sussman, Meteorologist, Fellow, American Meteorologist, fellow, American \n",
            "Meteorological Society. H Read McGrath, PhD, Meteorologist. Robert E. \n",
            "Zabrecky, Meteorologist.\n",
            "William  M. Porch, PhD, Atmospheric Physicist, Los Alamos national \n",
            "Laboratory. Earle R. Williams, PhD, Associate Profesor of Meteorology, Dept. \n",
            "of Earth, Atmospheric, and Planetary Sciences, Massachusetts Institute of \n",
            "Technology. S. Fred Singer, PhD, Atmospheric Physsicist, University of \n",
            "Virginia, Director, Science & Environmental Policy Project. (Affilitions \n",
            "listed are for identification purposes only).\n",
            "\n",
            "Document 4: SAMPE, NCGA, The University of Akron, and NASA Lewis Research Center\n",
            "is sponsoring:\n",
            "\n",
            "                      COMPUTERS AND COMPOSITES\n",
            "\n",
            "\tA one-day seminar devoted to practical applications of\n",
            "\tcomputer workstations for efficient processing, design, and\n",
            "\t\t\tManufacture of composites\n",
            "\n",
            "May 18, 1993\n",
            "at\n",
            " The University of Akron\n",
            "  Akron, Ohio\n",
            "\n",
            "Speakers on:\n",
            " Advancement in Graphics Visualization   Dr. Jay Horowitz, NASA\n",
            " Integrated Product Development with     Mr. Michael R. Cowen\n",
            "  Network Workstations\t\t          Sikorski Aircraft\n",
            " Structural Analysis\t\t\t Mr. Brian Fite, NASA\n",
            " Stereolithography\t\t\t Mr. Jason Williams, Penn State-Erie\n",
            " Molecular and Physical Modeling\t Dr. Vassilios Galiatsato,\n",
            "  of Polymer Curing                       University of Akron\n",
            " Process Modeling of Polymer\n",
            "  Matrix Composites\t\t\t Dr Ram Upadhyay, GE Corporate R&D\n",
            "\n",
            "Registration Fees: $75.00 Advance, $100.00 on site (Includes box lunch)\n",
            "\n",
            "Contact Gary Roberts, NASA Lewis Research Center (216) 433-344\n",
            "or write:\n",
            "\tSAMPE Regional Seminar\n",
            "\tc/o Gary Roberts\n",
            "\tNASA Lewis Research Center\n",
            "\t21000 Brookpark Rd MS 49-1\n",
            "\tCleveland, Ohio 44135\n",
            "\n",
            "Or Email to me, | and I'll get it to Gary.\n",
            "\t\t|\n",
            "\t        \\/\n",
            "Document 5: From the \"JPL Universe\"\n",
            "April 23, 1993\n",
            "\n",
            "VLBI project meets with international space agencies\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Evaluation\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import adjusted_rand_score,adjusted_mutual_info_score, normalized_mutual_info_score, completeness_score, homogeneity_score, v_measure_score, silhouette_score\n"
      ],
      "metadata": {
        "id": "UdTdZ3nML19A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform K-Means clustering on the LSI results\n",
        "n_clusters = 20  # Adjust this number based on your dataset\n",
        "kmeans = KMeans(n_clusters=n_clusters, random_state=0)\n",
        "clusters = kmeans.fit_predict(lsi_matrix)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d7XAtWQpL-JY",
        "outputId": "94b7e31a-d807-441f-8e69-37ba602177bc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Assuming you have actual labels for documents\n",
        "true_labels = newsgroups.target  # Replace with your actual labels\n",
        "\n",
        "# Calculate purity\n",
        "purity = accuracy_score(true_labels, clusters)\n",
        "print(f'Purity: {purity}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QEUAkVzPMBvL",
        "outputId": "612a2468-ad51-4c52-a625-8b2fe65983dd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Purity: 0.058898439987265204\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nmi = normalized_mutual_info_score(true_labels, clusters)\n",
        "print(f'Normalized Mutual Info Score(NMI): {nmi}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nI6DSf9vMEHr",
        "outputId": "548955a8-0878-42b5-dc25-5a956b648170"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NMI: 0.3131003278003342\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "silhouette_avg = silhouette_score(lsi_matrix, clusters)\n",
        "print(f'Silhouette Score: {silhouette_avg}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ItIl3YSMMGSi",
        "outputId": "0d3cd739-b6e1-42ac-810c-fa185a2e909e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Silhouette Score: 0.025945077776465663\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "completeness = completeness_score(true_labels, clusters)\n",
        "homogeneity = homogeneity_score(true_labels, clusters)\n",
        "v_measure = v_measure_score(true_labels, clusters)\n",
        "\n",
        "print(f'Completeness: {completeness}')\n",
        "print(f'Homogeneity: {homogeneity}')\n",
        "print(f'V-Measure: {v_measure}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V0OqGvohMJMD",
        "outputId": "2cf4e464-68d5-428d-a8bd-4825c300bc72"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Completeness: 0.34756798361468416\n",
            "Homogeneity: 0.2848520880546633\n",
            "V-Measure: 0.3131003278003342\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ari_score = adjusted_rand_score(true_labels, clusters)\n",
        "print(f\"Adjusted Rand Index (ARI): {ari_score}\")\n",
        "ami_score = adjusted_mutual_info_score(true_labels, clusters)\n",
        "print(f\"Adjusted Mutual Info Index (AMI): {ami_score}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4DscRxgN2rUu",
        "outputId": "265b07a6-c95d-4943-9407-f0a598d9d5fe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Adjusted Rand Index (ARI): 0.07192883408330963\n",
            "Adjusted Mutual Info Index (AMI): 0.31064911873418827\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Normalized Mutual Info Score(NMI): 0.3131003278003342\n",
        "Silhouette Score: 0.025945077776465663\n",
        "Completeness: 0.34756798361468416\n",
        "Homogeneity: 0.2848520880546633\n",
        "V-Measure: 0.3131003278003342\n",
        "Adjusted Rand Index (ARI): 0.07192883408330963\n",
        "Adjusted Mutual Info Index (AMI): 0.31064911873418827"
      ],
      "metadata": {
        "id": "1OVCUrS0BCpG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}